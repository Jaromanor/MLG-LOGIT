---
title: "mlr3_PIMA"
author: "¿?"
date: "2025-05-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# ENFOQUE
Un flujo moderno con mlr3, orientado a modelado predictivo.

Lo que vamos a lograr:

Preprocesamiento con imputación y escalado
Selección de variables (feature selection) tipo envoltura (wrapper)
Ajuste de hiperparámetros del modelo: No porque no hay opciones con GLM
Validación cruzada (CV) para entrenamiento y ajuste
Evaluación en un conjunto de testeo (test)
Explicación de cada paso, como para un informe claro
Análisis de resultados finales

# OTROS
En el contexto de machine learning, un "workflow robusto" significa que no solo estás entrenando un modelo una vez, sino que:

Validás el rendimiento con métodos como cross-validation
Ajustás hiperparámetros (hyperparameter tuning)
Comparás múltiples modelos con benchmarking

Hacés todo esto de forma modular, repetible y escalable
- Escogemos iterativamente las variables del modelo?
- Balanceamos la data?
- Ajustar hiperparámetros?. Ajustás hiperparámetros (hyperparameter tuning)

# CONTEXTO

En este script se van a trabajar los datos de PIMA pero tratando de usar una ecosistema MLR3 para tener un flujo de trabajo lo más estandar posible o estructurado. Esto es importante si se requiere reproducibilidad, validación cruzada y evaluación sólida de modelos, enfocado en la predicción y no en análisis o explicación, inferencia, o diagnóstico estructural.


```{r}
# Instala si no tienes

library(mlr3verse)
library(mlr3pipelines)
library(mlr3fselect)
library(mlbench)
library(dplyr)
library(mlr3measures)

```
 Cargamos y limpiamos la data:
```{r}
data(PimaIndiansDiabetes)

# Reemplazo de ceros no fisiológicos
df <- PimaIndiansDiabetes %>%
  mutate(across(c(glucose, pressure, triceps, insulin, mass), ~na_if(., 0)))
```

## Crear una TaskClassif

En mlr3, un Task representa un conjunto de datos junto con información contextual sobre el problema de aprendizaje automático que se desea resolver. Es una abstracción formal y estructurada que encapsula:

- los datos (como un data frame),
- el tipo de problema (clasificación, regresión, etc.),
- la variable objetivo,
- y opcionalmente, otras configuraciones (como la clase positiva, pesos, roles de las columnas, etc.).

Esto contrasta con otros frameworks como caret, que no usan una estructura formal como Task; simplemente pasan el data.frame, la variable objetivo como fórmula y el modelo, todo junto. Esto es más simple pero menos flexible y modular.

```{r}
# 1. Crear task con todas las variables
task <- TaskClassif$new(
  id = "diabetes",       # Nombre interno del task (útil para identificación y logs)
  backend = df,          # El data frame o DataTable con los datos
  target = "diabetes",   # La variable a predecir
  positive = "pos"       # (solo para clasificación binaria) clase positiva
)
```

Esta forma manual de modificar Task cuando quiero eliminar variables del modelo sin utilizar métodos de filtro "automatizados", digamos que por instinto el analista decide eliminar ciertas variables.

```{r}
# 2. Excluir variables específicas: Esto se hace porque tomando todos los datos
# algunas variables no resultaron significativas

variables_a_excluir <- c("insulin", "pregnant","pressure","triceps")
task$select(setdiff(task$feature_names, variables_a_excluir))

# Verificar
task$feature_names
```


## Dividir en entrenamiento y test

Se muestran 2 formas: ambas por ser de clasificación implícitamente realizan muestreo estratificado.

```{r}
set.seed(123)
split <- partition(task, ratio = 0.7)  # 70% train, 30% test
```

Este método es mejor porque se integra a otras tareas:

```{r}
set.seed(123)

resampling <- rsmp("holdout")
resampling$param_set$values$ratio <- 0.7
resampling$instantiate(task)

train_set <- resampling$train_set(1)
test_set  <- resampling$test_set(1)
```

## Análisis de desbalance

Es importante establecer que en este ejercicio nos la estamos jugando por un logit, por lo que se recomienda aplicar técnicas de balanceo antes del entrenamiento.

Pero ojo: Hay dos enfoques válidos, depende del objetivo:

| Enfoque                                  | ¿Cuándo usarlo?                                                                                                      |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| **Balancear antes de elegir el modelo**  | Cuando se sospecha que el desbalance afectará todos los modelos                                                      |
| **Probar modelos sin balancear primero** | Cuando se quieren comparar algoritmos en condiciones reales                                                          |
| **Balancear por modelo**                 | Cuando un modelo (como árboles) puede compensar el desbalance internamente, pero otros no (como regresión logística) |


```{r}
data_train<-df[train_set,]
```

```{r}
table(data_train$diabetes)
```

```{r}
prop.table(table(data_train$diabetes))
```

### Opción A: Sobremuestreo (po("classbalancing") con adjust = "minor" o "major")


```{r}
# Sobremuestreo de clase minoritaria
po_bal_oversample <- po("classbalancing",
                        id = "oversample",
                        adjust = "minor",
                        reference = "major",
                        shuffle = FALSE)
```


### Opción B: Submuestreo (adjust = "major")

```{r}
# Submuestreo de clase mayoritaria
po_bal_undersample <- po("classbalancing",
                         id = "undersample",
                         adjust = "major",
                         reference = "minor",
                         shuffle = FALSE)
```

A partir de aca vamos a ajustar el modelo con y sin datos balanceados para poder evaluar:


## Datos desbalanceados

### Armar pipeline con preprocesamiento + modelo

Este pipeline se encarga de imputar los valores faltantes automáticamente usando usando la media (algo no recomendable especialmente si hay valores extremos, hay otras técnicas), luego estandariza los datos (media 0, varianza 1), y finalmente aplica regresión logística.

Este pipeline se entrena con el train() en el conjunto de entrenamiento, y ese pipeline guarda internamente.

Por lo tanto, cuando luego haces predict(pipe, newdata = test_set), automáticamente:

Se imputan los NAs del test usando el modelo aprendido en train, y
Se escala el test con los parámetros del escalado aprendidos en train.

```{r}
# Imputación con árbol + escalado + modelo
pipe <- po("imputemean") %>>%
        po("scale") %>>%
        lrn("classif.log_reg", predict_type = "prob")

```
GraphLearner es una interfaz envoltorio (wrapper) en mlr3pipelines que convierte un pipeline (Graph) en un objeto tipo Learner, compatible con mlr3. Es decir:

Toma un pipeline de preprocesamiento y modelo(s), y lo convierte en algo que puede ser entrenado (train()) y evaluado (predict()) como cualquier otro Learner.

```{r}
# Convertimos en GraphLearner
learner <- GraphLearner$new(pipe)
```



### Selección de variables (envoltura)

Vamos a construir un learner que permita hacer selección de variables tipo envoltura sin ajuste de hiperparámetros.

```{r}
# Validación cruzada interna
resampling_inner <- rsmp("cv", folds = 5)

# Método de búsqueda: búsqueda aleatoria
fselector <- fs("random_search")

# Criterio de parada: 20 combinaciones
terminator <- trm("evals", n_evals = 20)

# AutoFSelector
auto_fs <- AutoFSelector$new(
  learner = learner,
  resampling = resampling_inner,
  measure = msr("classif.auc"),
  fselector = fselector,
  terminator = terminator
)
```

Este flujo automatiza:

Aquí usamos una búsqueda aleatoria para encontrar el mejor subconjunto de variables predictoras, evaluando cada combinación con validación cruzada de 5 pliegues, optimizando el AUC.


### Entrenar con selección de variables
Aquí es donde se corre el método de selección de variables.
```{r}
auto_fs$train(task, row_ids = split$train)
```
ocurre lo siguiente:

1. Se inicializa el FSelector (como random_search).

2. Se generan distintos subconjuntos de variables.

3. Para cada subconjunto:

4. Se entrena el learner con split$train.

5. Se evalúa con el esquema de resampling interno (como CV).

6. El mejor subconjunto de variables se selecciona según la métrica.

7. El AutoFSelector entrena el modelo final con ese subconjunto óptimo, usando los datos de split$train.


### Ver resultados del mejor modelo
Podemos ver qué variables quedaron.

```{r}
# Variables seleccionadas
auto_fs$fselect_result
```


### Evaluación en el conjunto de prueba

Esto mide el desempeño real del modelo en datos que nunca vio: es la evaluación honesta. Las métricas nos dicen cuán bien predice la diabetes: sensibilidad, especificidad, precisión y AUC. Para ello se van a realizar los siguientes pasos:

1. Predicción sobre el test

Se crea una especie de marco de datos:
- row_ids: identificador de las observaciones del set test.
- truth: clasificación real de los datos.
- prob.pos o prob.neg: Basado en el modelo estimado muestra la probabilidad de que sea positivo o negativo.
- response: Clasificación basada en la probabilidad estimada, en este caso toma 0.5 como probabilidad, si prob.pos > 0.5 se asume "pos"



```{r}

# Predicción en datos de test
prediction <- auto_fs$predict(task, row_ids = split$test)
```

2. Construcción de la curva ROC y cálculo del umbral: Esto se hace para generar las métricas con el umbral óptimo y después ajustarlo de acuerdo con las necesidades o interés de la prueba, es decir, queremos aumentar o disminuir los falsos positivos?

```{r}
library(mlr3viz)
library(ggplot2)  # para customizar si quieres

autoplot(prediction, type = "roc") +
  ggtitle("Curva ROC - Datos de Test") +
  theme_minimal()
prediction$score(msr("classif.auc"))
```

Finalmente, calculamos las métricas para evaluar el modelo basado en umbral 0.5

```{r}
#cálculo de las métricas
prediction$score(msrs(c(
  "classif.acc",
  "classif.sensitivity",
  "classif.specificity",
  "classif.precision",
  "classif.fbeta"
)))
```


Ahora bien, lo correcto es revisar el modelo en el contexto que queremos analizar. ¿Cuál es el interés de la prueba, por ejemplo que tan importante es diagnósticar falsamente que el paciente tiene diabetes frente a diagnosticar falsamente que está sano.

Es en esta decisión donde se define el umbral más oportuno, en este caso, no nos importa aumentar los Falsos Positivos, es decir, no queremos que un paciente enfermo termine con un resultado negativo.

Pero cuánto debe ser ese umbral?

Verificamos el umbral óptimo de la siguiente manera aclarando que este es punto de corte (threshold) que maximiza la diferencia entre la tasa de verdaderos positivos (sensibilidad) y la tasa de falsos positivos (1 - especificidad).

```{r}
library(pROC)

# Probabilidades para la clase positiva
prob_pos <- prediction$prob[, "pos"]

# Convertir etiquetas reales a 0/1
truth <- prediction$truth

row_ids <- prediction$row_ids


# Crear curva ROC
roc_obj <- roc(response = truth, predictor = prob_pos)

opt_coords <- coords(roc_obj, "best", best.method = "youden", ret = c("threshold", "sensitivity", "specificity"))
opt_threshold <- as.numeric(opt_coords["threshold"])
opt_threshold

```
Se establece que el umbral óptimo es 0.2805812. El índice de Youden (J = Sensibilidad + Especificidad - 1) busca el threshold que maximiza la capacidad del modelo para distinguir entre clases, evitando sesgos hacia sensibilidad o especificidad extremas. Con este umbral van aumentar los FP


3. Clasificar con el umbral personalizado


```{r}
# Reclasificación manual
predicted_custom <- ifelse(prob_pos >= opt_threshold, "pos", "neg")
```

```{r}
predicted_custom <- factor(predicted_custom, levels = levels(truth))
```


```{r}
prediction_custom <- mlr3::PredictionClassif$new(
  truth = truth,
  response = predicted_custom,
  prob = prediction$prob,
  row_ids = prediction$row_ids
)
```

3.1. Cálculo de métricas

```{r}
prediction_custom$score(msrs(c(
  "classif.acc",
  "classif.sensitivity",
  "classif.specificity",
  "classif.precision",
  "classif.fbeta"
)))
```

3.2. Matriz de confusión y curva roc (roc con datos de test)

```{r}
# Matriz de confusión
prediction_custom$confusion

library(mlr3viz)
library(ggplot2)  # para customizar si quieres

autoplot(prediction_custom, type = "roc") +
  ggtitle("Curva ROC - Datos de Test") +
  theme_minimal()
prediction$score(msr("classif.auc"))

```

La sensibilidad pasa de 0.53 (umbral 0.5) a 0.81 (umbral óptimo). Se recomienda bajar el umbral por el propósito de la prueba.





## Con Balanceo


### Armar pipeline con preprocesamiento + modelo


```{r}
# Imputación con árbol + escalado + modelo
pipe <- po_bal_oversample %>>%
  po("imputelearner", learner = lrn("regr.rpart")) %>>%
        po("scale") %>>%
        lrn("classif.log_reg", predict_type = "prob")

```


```{r}
# Convertimos en GraphLearner
learner <- GraphLearner$new(pipe)
```

### Selección de variables (envoltura)

```{r}
# Validación cruzada interna
resampling_inner <- rsmp("cv", folds = 5)

# Método de búsqueda: búsqueda aleatoria
fselector <- fs("random_search")

# Criterio de parada: 20 combinaciones
terminator <- trm("evals", n_evals = 20)

# AutoFSelector
auto_fs <- AutoFSelector$new(
  learner = learner,
  resampling = resampling_inner,
  measure = msr("classif.auc"),
  fselector = fselector,
  terminator = terminator
)
```


### Entrenar con selección de variables
Aquí es donde se corre el método de selección de variables.
```{r}
auto_fs$train(task, row_ids = split$train)
```

### Ver resultados del mejor modelo
Podemos ver qué variables quedaron.

```{r}
# Variables seleccionadas
auto_fs$fselect_result
```


### Evaluación en el conjunto de prueba

Esto mide el desempeño real del modelo en datos que nunca vio: es la evaluación honesta. Las métricas nos dicen cuán bien predice la diabetes: sensibilidad, especificidad, precisión y AUC. Para ello se van a realizar los siguientes pasos:

1. Predicción sobre el test

```{r}

# Predicción en datos de test
prediction <- auto_fs$predict(task, row_ids = split$test)
```

2. Construcción de la curva ROC y cálculo del umbral: Esto se hace para generar las métricas con el umbral óptimo y después ajustarlo de acuerdo con las necesidades o interés de la prueba, es decir, queremos aumentar o disminuir los falsos positivos?

```{r}
library(mlr3viz)
library(ggplot2)  # para customizar si quieres

autoplot(prediction, type = "roc") +
  ggtitle("Curva ROC - Datos de Test") +
  theme_minimal()
prediction$score(msr("classif.auc"))
```
Umbral:

```{r}
library(pROC)

# Probabilidades para la clase positiva
prob_pos <- prediction$prob[, "pos"]

# Convertir etiquetas reales a 0/1
truth <- prediction$truth

row_ids    <- prediction$row_ids


# Crear curva ROC
roc_obj <- roc(response = truth, predictor = prob_pos)

opt_coords <- coords(roc_obj, "best", best.method = "youden", ret = c("threshold", "sensitivity", "specificity"))
opt_threshold <- as.numeric(opt_coords["threshold"])
opt_threshold

```

3. Clasificar con el umbral personalizado


```{r}
# Reclasificación manual
predicted_custom <- ifelse(prob_pos >= opt_threshold, "pos", "neg")
```

```{r}
predicted_custom <- factor(predicted_custom, levels = levels(truth))
```


```{r}
prediction_custom <- mlr3::PredictionClassif$new(
  truth = truth,
  response = predicted_custom,
  prob = prediction$prob,
  row_ids = prediction$row_ids
)
```

4. Cálculo de métricas

```{r}
prediction_custom$score(msrs(c(
  "classif.acc",
  "classif.sensitivity",
  "classif.specificity",
  "classif.precision",
  "classif.fbeta"
)))
```

5. Matriz de confusión y curva roc (roc con datos de test)

```{r}
# Matriz de confusión
prediction$confusion

library(mlr3viz)
library(ggplot2)  # para customizar si quieres

autoplot(prediction, type = "roc") +
  ggtitle("Curva ROC - Datos de Test") +
  theme_minimal()
prediction$score(msr("classif.auc"))

```

## Con Balanceo





























## Acceder al modelo glm para inferencia
```{r}
modelo_final <- auto_fs$learner$model$classif.log_reg$model

# Análisis inferencial
summary(modelo_final)
```

Este flujo completo cumple con los estándares modernos de ciencia de datos, incluyendo:

Imputación automática y escalado

Selección de variables basada en desempeño: el modelo redujo la dimensionalidad a las variables más informativas según el AUC. Esto ayuda a evitar el sobreajuste y mejora la interpretabilidad.

Evaluación cruzada robusta: el proceso de selección interna asegura que el conjunto final tiene buen desempeño en entrenamiento.

Análisis inferencial con glm()

Evaluación real sobre datos nuevos

Este tipo de pipeline es ideal cuando se quiere predecir bien sin sacrificar la posibilidad de interpretar el modelo y entender las variables más importantes.






































